{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/learn/generation/langchain/rag-chatbot.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/learn/generation/langchain/rag-chatbot.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building RAG Chatbots with LangChain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we'll work on building an AI chatbot from start-to-finish. We will be using LangChain, OpenAI, and Pinecone vector DB, to build a chatbot capable of learning from the external world using **R**etrieval **A**ugmented **G**eneration (RAG).\n",
    "\n",
    "We will be using a dataset sourced from the Llama 2 ArXiv paper and other related papers to help our chatbot answer questions about the latest and greatest in the world of GenAI.\n",
    "\n",
    "By the end of the example we'll have a functioning chatbot and RAG pipeline that can hold a conversation and provide informative responses based on a knowledge base.\n",
    "\n",
    "### Before you begin\n",
    "\n",
    "You'll need to get an [OpenAI API key](https://platform.openai.com/account/api-keys) and [Pinecone API key](https://app.pinecone.io)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start building our chatbot, we need to install some Python libraries. Here's a brief overview of what each library does:\n",
    "\n",
    "- **langchain**: This is a library for GenAI. We'll use it to chain together different language models and components for our chatbot.\n",
    "- **openai**: This is the official OpenAI Python client. We'll use it to interact with the OpenAI API and generate responses for our chatbot.\n",
    "- **datasets**: This library provides a vast array of datasets for machine learning. We'll use it to load our knowledge base for the chatbot.\n",
    "- **pinecone-client**: This is the official Pinecone Python client. We'll use it to interact with the Pinecone API and store our chatbot's knowledge base in a vector database.\n",
    "\n",
    "You can install these libraries using pip like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "flagembedding 1.1.8 requires transformers==4.34.0, but you have transformers 4.36.2 which is incompatible.\n",
      "ragas 0.0.22 requires openai>1, but you have openai 0.28.0 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip install -qU \\\n",
    "#     langchain==0.0.292 \\\n",
    "#     openai==0.28.0 \\\n",
    "#     datasets==2.10.1 \\\n",
    "#     pinecone-client==2.2.4 \\\n",
    "#     tiktoken==0.5.1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Chatbot (no RAG)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be relying heavily on the LangChain library to bring together the different components needed for our chatbot. To begin, we'll create a simple chatbot without any retrieval augmentation. We do this by initializing a `ChatOpenAI` object. For this we do need an [OpenAI API key](https://platform.openai.com/account/api-keys)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "OPENAI_API_KEY=os.getenv(\"OPENAI_API_KEY\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elias/miniconda/envs/rag1/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "OPENAI_API_KEY= os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") \n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    model='gpt-3.5-turbo'\n",
    ")\n",
    "# print(OPENAI_API_KEY) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chats with OpenAI's `gpt-3.5-turbo` and `gpt-4` chat models are typically structured (in plain text) like this:\n",
    "\n",
    "```\n",
    "System: You are a helpful assistant.\n",
    "\n",
    "User: Hi AI, how are you today?\n",
    "\n",
    "Assistant: I'm great thank you. How can I help you?\n",
    "\n",
    "User: I'd like to understand string theory.\n",
    "\n",
    "Assistant:\n",
    "```\n",
    "\n",
    "The final `\"Assistant:\"` without a response is what would prompt the model to continue the conversation. In the official OpenAI `ChatCompletion` endpoint these would be passed to the model in a format like:\n",
    "\n",
    "```python\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hi AI, how are you today?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I'm great thank you. How can I help you?\"}\n",
    "    {\"role\": \"user\", \"content\": \"I'd like to understand string theory.\"}\n",
    "]\n",
    "```\n",
    "\n",
    "In LangChain there is a slightly different format. We use three _message_ objects like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Hi AI, how are you today?\"),\n",
    "    AIMessage(content=\"I'm great thank you. How can I help you?\"),\n",
    "    HumanMessage(content=\"I'd like to understand string theory.\")\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The format is very similar, we're just swapped the role of `\"user\"` for `HumanMessage`, and the role of `\"assistant\"` for `AIMessage`.\n",
    "\n",
    "We generate the next response from the AI by passing these messages to the `ChatOpenAI` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elias/miniconda/envs/rag1/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Sure, I can help you with that. String theory is a theoretical framework in physics that aims to describe the fundamental particles and forces of nature. It suggests that at the most fundamental level, everything in the universe is made up of tiny vibrating strings or loops of energy.\\n\\nAccording to string theory, these strings are incredibly small, with a length scale of about the Planck length (about 10^(-35) meters). The different vibrational patterns of these strings give rise to different particles, such as electrons, quarks, and photons.\\n\\nOne of the key ideas in string theory is that it requires more than the usual four dimensions of space and time (three spatial dimensions and one time dimension) that we are familiar with. In fact, string theory suggests that there are additional hidden dimensions, usually referred to as \"extra dimensions,\" which are curled up or compactified at such small scales that we can\\'t perceive them directly.\\n\\nThese extra dimensions can take various forms, such as being shaped like a tiny loop or curled up in intricate ways. The number and shape of these extra dimensions have a significant impact on the properties of particles and the forces between them.\\n\\nString theory also proposes that there are different versions or \"string theories,\" which include various types of strings and interactions. These different versions are unified by a principle known as \"duality,\" which shows that seemingly different versions of string theory are mathematically equivalent descriptions of the same underlying physics.\\n\\nIt\\'s important to note that string theory is still a work in progress and remains highly theoretical. It has not yet been experimentally proven, and many aspects of the theory are still being explored and understood by physicists.\\n\\nI hope this gives you a basic understanding of string theory. Let me know if you have any more questions!')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = chat(messages)\n",
    "res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In response we get another AI message object. We can print it more clearly like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, I can help you with that. String theory is a theoretical framework in physics that aims to describe the fundamental particles and forces of nature. It suggests that at the most fundamental level, everything in the universe is made up of tiny vibrating strings or loops of energy.\n",
      "\n",
      "According to string theory, these strings are incredibly small, with a length scale of about the Planck length (about 10^(-35) meters). The different vibrational patterns of these strings give rise to different particles, such as electrons, quarks, and photons.\n",
      "\n",
      "One of the key ideas in string theory is that it requires more than the usual four dimensions of space and time (three spatial dimensions and one time dimension) that we are familiar with. In fact, string theory suggests that there are additional hidden dimensions, usually referred to as \"extra dimensions,\" which are curled up or compactified at such small scales that we can't perceive them directly.\n",
      "\n",
      "These extra dimensions can take various forms, such as being shaped like a tiny loop or curled up in intricate ways. The number and shape of these extra dimensions have a significant impact on the properties of particles and the forces between them.\n",
      "\n",
      "String theory also proposes that there are different versions or \"string theories,\" which include various types of strings and interactions. These different versions are unified by a principle known as \"duality,\" which shows that seemingly different versions of string theory are mathematically equivalent descriptions of the same underlying physics.\n",
      "\n",
      "It's important to note that string theory is still a work in progress and remains highly theoretical. It has not yet been experimentally proven, and many aspects of the theory are still being explored and understood by physicists.\n",
      "\n",
      "I hope this gives you a basic understanding of string theory. Let me know if you have any more questions!\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because `res` is just another `AIMessage` object, we can append it to `messages`, add another `HumanMessage`, and generate the next response in the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physicists believe that string theory has the potential to provide a unified theory because it has the ability to incorporate all the fundamental forces of nature into a single framework. Currently, there are four known fundamental forces: gravity, electromagnetism, and the strong and weak nuclear forces.\n",
      "\n",
      "One of the challenges in physics has been to explain how these forces are related and to develop a theory that can describe them all consistently. String theory suggests that all particles and forces arise from the vibrations of tiny strings, which provides a potential framework for unification.\n",
      "\n",
      "In string theory, the different vibrational modes of the strings correspond to different particles and their interactions. As a result, it is possible to describe not only the particles of matter but also the particles that mediate the forces between them, such as photons for electromagnetic interactions and gravitons for gravity.\n",
      "\n",
      "Moreover, string theory also predicts the existence of additional particles, such as supersymmetric partners, which could help explain some of the discrepancies and open questions in our current understanding of particle physics.\n",
      "\n",
      "Additionally, string theory offers the possibility of incorporating gravity into a quantum mechanical framework, which is currently a major challenge in theoretical physics. By combining quantum mechanics and general relativity (the theory of gravity), string theory aims to provide a consistent description of the universe at both the microscopic and macroscopic scales.\n",
      "\n",
      "While string theory shows promise for unifying the fundamental forces, it is important to note that it is still a work in progress. Many aspects of the theory are still being explored, and there is ongoing research to develop a more complete and experimentally testable version of string theory.\n",
      "\n",
      "In summary, physicists believe that string theory has the potential to provide a unified theory by incorporating all the fundamental forces into a single framework and by offering a way to reconcile quantum mechanics and general relativity. However, further research and experimentation are needed to confirm and refine the theory.\n"
     ]
    }
   ],
   "source": [
    "# add latest AI response to messages\n",
    "messages.append(res)\n",
    "\n",
    "# now create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=\"Why do physicists believe it can produce a 'unified theory'?\"\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to chat-gpt\n",
    "res = chat(messages)\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Hallucinations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our chatbot, but as mentioned — the knowledge of LLMs can be limited. The reason for this is that LLMs learn all they know during training. An LLM essentially compresses the \"world\" as seen in the training data into the internal parameters of the model. We call this knowledge the _parametric knowledge_ of the model.\n",
    "\n",
    "By default, LLMs have no access to the external world.\n",
    "\n",
    "The result of this is very clear when we ask LLMs about more recent information, like about the new (and very popular) Llama 2 LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add latest AI response to messages\n",
    "messages.append(res)\n",
    "\n",
    "# now create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=\"What is so special about Llama 2?\"\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to OpenAI\n",
    "res = chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but I'm not aware of any specific significance or special meaning associated with \"Llama 2.\" It's possible that you may be referring to something specific, like a book, a movie, a game, or a particular reference in a context that I'm not aware of. Could you please provide more information or context so that I can better understand and assist you?\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our chatbot can no longer help us, it doesn't contain the information we need to answer the question. It was very clear from this answer that the LLM doesn't know the informaiton, but sometimes an LLM may respond like it _does_ know the answer — and this can be very hard to detect.\n",
    "\n",
    "OpenAI have since adjusted the behavior for this particular example as we can see below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add latest AI response to messages\n",
    "messages.append(res)\n",
    "\n",
    "# now create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=\"Can you tell me about the LLMChain in LangChain?\"\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to OpenAI\n",
    "res = chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I apologize, but I couldn't find any specific information about an \"LLMChain\" in relation to \"LangChain.\" It's possible that these terms are specific to a particular context or project that I'm not familiar with. If you can provide more details or clarify the context, I'll do my best to assist you further.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is another way of feeding knowledge into LLMs. It is called _source knowledge_ and it refers to any information fed into the LLM via the prompt. We can try that with the LLMChain question. We can take a description of this object from the LangChain documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "llmchain_information = [\n",
    "    \"A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format.\",\n",
    "    \"Chains is an incredibly generic concept which returns to a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case.\",\n",
    "    \"LangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data, (2) Be agentic: Allow a language model to interact with its environment. As such, the LangChain framework is designed with the objective in mind to enable those types of applications.\"\n",
    "]\n",
    "\n",
    "source_knowledge = \"\\n\".join(llmchain_information)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can feed this additional knowledge into our prompt with some instructions telling the LLM how we'd like it to use this information alongside our original query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Can you tell me about the LLMChain in LangChain?\"\n",
    "\n",
    "augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
    "\n",
    "Contexts:\n",
    "{source_knowledge}\n",
    "\n",
    "Query: {query}\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we feed this into our chatbot as we were before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=augmented_prompt\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to OpenAI\n",
    "res = chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The LLMChain is a specific type of chain within the LangChain framework, which is used for developing applications powered by language models. A chain in this context refers to a sequence of modular components combined in a specific way to achieve a particular purpose.\n",
      "\n",
      "The LLMChain, in particular, is the most common type of chain in LangChain. It consists of three main components: a PromptTemplate, a language model (either an LLM or a ChatModel), and an optional output parser. \n",
      "\n",
      "When using an LLMChain, multiple input variables are taken and formatted into a prompt using the PromptTemplate. This prompt is then passed to the language model, which generates an output based on the given input. Finally, the optional output parser can be used to parse and format the output of the language model into a final desired format.\n",
      "\n",
      "The LangChain framework aims to enable powerful and differentiated applications by not only integrating language models through an API but also by making them data-aware and agentic. Being data-aware allows the language model to connect with other sources of data, while being agentic enables the language model to interact with its environment.\n",
      "\n",
      "In summary, the LLMChain is a fundamental component of the LangChain framework, providing a way to structure and utilize language models within applications, with support for input formatting, language model processing, and output parsing.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quality of this answer is phenomenal. This is made possible thanks to the idea of augmented our query with external knowledge (source knowledge). There's just one problem — how do we get this information in the first place?\n",
    "\n",
    "We learned in the previous chapters about Pinecone and vector databases. Well, they can help us here too. But first, we'll need a dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, we will be importing our data. We will be using the Hugging Face Datasets library to load our data. Specifically, we will be using the `\"jamescalam/llama-2-arxiv-papers\"` dataset. This dataset contains a collection of ArXiv papers which will serve as the external knowledge base for our chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elias/miniconda/envs/rag1/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['doi', 'chunk-id', 'chunk', 'id', 'title', 'summary', 'source', 'authors', 'categories', 'comment', 'journal_ref', 'primary_category', 'published', 'updated', 'references'],\n",
       "    num_rows: 4838\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"jamescalam/llama-2-arxiv-papers-chunked\",\n",
    "    split=\"train\"\n",
    ")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doi': '1102.0183',\n",
       " 'chunk-id': '0',\n",
       " 'chunk': 'High-Performance Neural Networks\\nfor Visual Object Classi\\x0ccation\\nDan C. Cire\\x18 san, Ueli Meier, Jonathan Masci,\\nLuca M. Gambardella and J\\x7f urgen Schmidhuber\\nTechnical Report No. IDSIA-01-11\\nJanuary 2011\\nIDSIA / USI-SUPSI\\nDalle Molle Institute for Arti\\x0ccial Intelligence\\nGalleria 2, 6928 Manno, Switzerland\\nIDSIA is a joint institute of both University of Lugano (USI) and University of Applied Sciences of Southern Switzerland (SUPSI),\\nand was founded in 1988 by the Dalle Molle Foundation which promoted quality of life.\\nThis work was partially supported by the Swiss Commission for Technology and Innovation (CTI), Project n. 9688.1 IFF:\\nIntelligent Fill in Form.arXiv:1102.0183v1  [cs.AI]  1 Feb 2011\\nTechnical Report No. IDSIA-01-11 1\\nHigh-Performance Neural Networks\\nfor Visual Object Classi\\x0ccation\\nDan C. Cire\\x18 san, Ueli Meier, Jonathan Masci,\\nLuca M. Gambardella and J\\x7f urgen Schmidhuber\\nJanuary 2011\\nAbstract\\nWe present a fast, fully parameterizable GPU implementation of Convolutional Neural\\nNetwork variants. Our feature extractors are neither carefully designed nor pre-wired, but',\n",
       " 'id': '1102.0183',\n",
       " 'title': 'High-Performance Neural Networks for Visual Object Classification',\n",
       " 'summary': 'We present a fast, fully parameterizable GPU implementation of Convolutional\\nNeural Network variants. Our feature extractors are neither carefully designed\\nnor pre-wired, but rather learned in a supervised way. Our deep hierarchical\\narchitectures achieve the best published results on benchmarks for object\\nclassification (NORB, CIFAR10) and handwritten digit recognition (MNIST), with\\nerror rates of 2.53%, 19.51%, 0.35%, respectively. Deep nets trained by simple\\nback-propagation perform better than more shallow ones. Learning is\\nsurprisingly rapid. NORB is completely trained within five epochs. Test error\\nrates on MNIST drop to 2.42%, 0.97% and 0.48% after 1, 3 and 17 epochs,\\nrespectively.',\n",
       " 'source': 'http://arxiv.org/pdf/1102.0183',\n",
       " 'authors': ['Dan C. Cireşan',\n",
       "  'Ueli Meier',\n",
       "  'Jonathan Masci',\n",
       "  'Luca M. Gambardella',\n",
       "  'Jürgen Schmidhuber'],\n",
       " 'categories': ['cs.AI', 'cs.NE'],\n",
       " 'comment': '12 pages, 2 figures, 5 tables',\n",
       " 'journal_ref': None,\n",
       " 'primary_category': 'cs.AI',\n",
       " 'published': '20110201',\n",
       " 'updated': '20110201',\n",
       " 'references': []}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Overview\n",
    "\n",
    "The dataset we are using is sourced from the Llama 2 ArXiv papers. It is a collection of academic papers from ArXiv, a repository of electronic preprints approved for publication after moderation. Each entry in the dataset represents a \"chunk\" of text from these papers.\n",
    "\n",
    "Because most **L**arge **L**anguage **M**odels (LLMs) only contain knowledge of the world as it was during training, they cannot answer our questions about Llama 2 — at least not without this data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Building the Knowledge Base"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a dataset that can serve as our chatbot knowledge base. Our next task is to transform that dataset into the knowledge base that our chatbot can use. To do this we must use an embedding model and vector database.\n",
    "\n",
    "We begin by initializing our connection to Pinecone, this requires a [free API key](https://app.pinecone.io)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pinecone\n",
    "\n",
    "# get API key from app.pinecone.io and environment from console\n",
    "pinecone.init(\n",
    "    api_key=os.environ.get('PINECONE_API_KEY') or 'PINECONE_API_KEY',\n",
    "    environment=os.environ.get('PINECONE_ENVIRONMENT') or 'gcp-starter'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we initialize the index. We will be using OpenAI's `text-embedding-ada-002` model for creating the embeddings, so we set the `dimension` to `1536`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10 Academy Cohort A\\nWeekly Challenge: Week 6\\nPrecision RAG: Prompt Tuning For\\nBuilding Enterprise Grade RAG\\nSystems\\nBusiness objective\\nPromptlyTech is an innovative e-business specializing in providing AI-driven solutions for\\noptimizing the use of Language Models (LLMs) in various industries. The company aims\\nto revolutionize how businesses interact with LLMs, making the technology more\\naccessible, eﬃcient, and eﬀective. By addressing the challenges of prompt engineering,\\nthe company plays a pivotal role in enhancing decision-making, operational eﬃciency,\\nand customer\\nexperience across various industries. PromptlyTech\\'s solutions are\\ndesigned to cater to the evolving needs of a digitally-driven business landscape, where\\nspeed and accuracy are key to staying competitive.\\nThe company focuses on key services: Automatic Prompt Generation, Automatic\\nEvaluation Data Generation, and Prompt Testing and Ranking.\\n1. Automatic Prompt Generation Service:\\n●\\nThis service streamlines the process of creating eﬀective prompts, enabling\\nbusinesses to eﬃciently utilize LLMs for generating high-quality, relevant content.\\nIt signiﬁcantly reduces the time and expertise required in crafting prompts\\nmanually.\\n2. Automatic Evaluation Data Generation Service:\\n●\\nPromptlyTech’s service automates the generation of diverse test cases, ensuring\\ncomprehensive coverage and identifying potential issues. This enhances the\\nreliability and performance of LLM applications, saving signiﬁcant time in the\\nQA(Quality Assurance) process.\\n3. Prompt Testing and Ranking Service:\\n●\\nPromptlyTech’s service evaluates and ranks diﬀerent prompts based on\\neﬀectiveness, helping Users to get the desired outcome from LLM. It ensures that\\nchatbots and virtual assistants provide accurate, contextually relevant responses,\\nthereby improving user engagement and satisfaction.\\nBackground Context\\nIn the evolving ﬁeld of artiﬁcial intelligence, Language Models (LLMs) like GPT-3.5 and\\nGPT-4 have become crucial for various applications. Their eﬀectiveness, however, heavily\\ndepends on the quality of the prompts they receive, leading to the emergence of\\n\"prompt engineering\" as a key skill.\\nPrompt engineering is the craft of designing queries or statements to guide LLMs to\\nproduce desired outcomes. The challenge lies in the sensitivity of these models to\\nprompt nuances, where slight variations can yield vastly diﬀerent results. This poses a\\nsigniﬁcant hurdle for users, especially in business contexts where accuracy and\\nrelevance are paramount.\\nThe need for simpliﬁed, eﬃcient prompt engineering is clear. Automating and optimizing\\nthis process can save time, enhance LLM productivity, and make advanced AI\\ncapabilities more accessible to a broader range of users. The tasks of Automatic Prompt\\nGeneration, Evaluation Data Generation, and Prompt Testing and Ranking are aimed at\\naddressing these challenges, streamlining the prompt engineering process for more\\neﬀective use of LLMs.\\nLearning Outcomes\\nSkills Development\\n●\\nPrompt Engineering Proﬁciency: Gain expertise in crafting eﬀective prompts that\\nguide LLMs to desired outputs, understanding nuances and variations in\\nlanguage that impact model responses.\\n●\\nCritical Analysis: Develop the ability to critically analyze and evaluate the\\neﬀectiveness of diﬀerent prompts based on their performance in varied scenarios.\\n●\\nTechnical Aptitude with LLMs: Enhance technical skills in using advanced\\nlanguage models like GPT-4 and GPT-3.5-Turbo, understanding their\\nfunctionalities and capabilities.\\n●\\nProblem-Solving and Creativity: Cultivate creative problem-solving skills by\\ngenerating innovative prompts and test cases, addressing complex and varied\\nobjectives.\\n●\\nData Interpretation: Learn to interpret and analyze data from test cases and\\nprompt evaluations, deriving meaningful insights from performance metrics.\\nKnowledge Acquisition\\n●\\nUnderstanding of Language Models: Acquire a deeper understanding of how\\nLLMs function, including their strengths, limitations, and the principles behind\\ntheir responses.\\n●\\nInsights into Automated Evaluation Data Generation: Gain knowledge about the\\nmethodology and importance of creating test cases for evaluating prompt\\neﬀectiveness.\\n●\\nELO Rating System and its Applications: Learn about the ELO rating system used\\nfor ranking prompts, understanding its mechanics and relevance in performance\\nevaluation.\\n●\\nPrompt Optimization Strategies: Understand various strategies for reﬁning and\\noptimizing prompts to achieve better alignment with speciﬁc goals and desired\\noutcomes.\\n●\\nIndustry Best Practices: Familiarize with the best practices in prompt engineering\\nwithin diﬀerent industries, learning about real-world applications and challenges.\\nTeam\\nTutors:\\n-\\nYabebal\\n-\\nEmitinan\\n-\\nRehmet\\nBadges\\nEach week, one user will be awarded one of the badges below for the best performance\\nin the category below.\\nIn addition to being the badge holder for that badge, each badge winner will get +20\\npoints to the overall score.\\nVisualization - quality of visualizations, understandability, skimmability, choice of\\nvisualization\\nQuality of code - reliability, maintainability, eﬃciency, commenting - in future this\\nwill be CICD/CML\\nInnovative approach to analysis -using latest algorithms, adding in research\\npaper content and other innovative approaches\\nWriting and presentation - clarity of written outputs, clarity of slides, overall\\nproduction value\\nMost supportive in the community - helping others, adding links, tutoring those\\nstruggling\\nThe goal of this approach is to support and reward expertise in diﬀerent parts of the\\nMachine learning engineering toolbox.\\nGroup Work Policy\\nEveryone has to submit all their work individually.\\nInstruction: Automatic Prompt Engineering\\nFundamental Tasks\\nThe core tasks for this week’s challenge in Automatic Prompt Engineering are outlined\\nbelow:\\n1.\\nUnderstand Prompt Engineering Tools and Concepts: Gain a thorough\\nunderstanding of the tools and theoretical concepts involved in prompt\\nengineering for Language Models (LLMs).\\n2.\\nFamiliarize with Language Models: Learn about the capabilities and functionalities\\nof advanced LLMs like GPT-4 and GPT-3.5-Turbo.\\n3.\\nDevelop a Plan for Prompt Generation and Testing: Create a comprehensive plan\\nthat outlines the approach for automated prompt generation, test case creation,\\nand prompt evaluation.\\n4.\\nSet Up a Development Environment: Prepare a suitable development\\nenvironment that supports the integration and testing of LLMs in the prompt\\nengineering process.\\n5.\\nDesign User Interface for Prompt System: Plan and initiate the development of a\\nuser-friendly interface for prompt input, reﬁnement, and performance analysis.\\n6.\\nPlan Integration of LLMs: Strategize the integration of LLMs into the prompt\\nsystem for automated generation and testing.\\n7.\\nBuild and Reﬁne Prompt Generation System: Develop the automated prompt\\ngeneration system, ensuring it aligns with user inputs and objectives.\\n8.\\nDevelop Automatic Evaluation Data Generation System: Create a system for\\ngenerating test cases that evaluate the eﬀectiveness of prompts in various\\nscenarios.\\n9.\\nImplement Prompt Testing and Evaluation Mechanism: Set up testing procedures\\nusing Monte Carlo matchmaking and ELO rating systems to evaluate and rank\\nprompts.\\n10. Reﬁne and Optimize System Based on Feedback: Continuously reﬁne the prompt\\ngeneration and evaluation system based on user feedback and performance data.\\nTask 1: Review the Evolution of Automatic Prompt Engineering\\nFocus on understanding the key developments in the ﬁeld of automatic prompt\\nengineering for Language Models (LLMs).\\nStudy Key Concepts and Tools:\\n●\\nUnderstand the key components of an enterprise grade RAG systems\\n○\\nRetrieval-augmented generation (RAG): What it is and why it’s a hot topic\\nfor enterprise AI\\n○\\nAdvanced RAG for LLMs/SLMs\\n○\\nRAG for Text Generation Processes in Businesses (check part 1, 3, & 4 as\\nwell)\\n○\\nLangchain Reterivers\\n●\\nUnderstand the need for advanced prompt engineering in building enterprise\\ngrade RAG systems\\n○\\nFull Fine-Tuning, PEFT, Prompt Engineering, and RAG: Which One Is Right\\nfor You?\\n○\\nAdvanced Prompt Engineering - Practical Examples\\n○\\nPrompt Engineering 201: Advanced methods and toolkits\\n○\\nDo you agree with this article? RAG is Just Fancier Prompt Engineering\\n●\\nUnderstand the need for evaluating RAG components\\n○\\nAn Overview on RAG Evaluation\\n○\\nEvaluating RAG: Using LLMs to Automate Benchmarking of Retrieval\\nAugmented Generation Systems\\n○\\nEvaluating RAG Applications with RAGAs\\n○\\nRAG Evaluation Using LangChain and Ragas\\n○\\nRAG System: Metrics and Evaluation Analysis with LlamaIndex\\n○\\nEvaluating RAG Part I: How to Evaluate Document Retrieval\\n○\\nEvaluating RAG/LLMs in highly technical settings using synthetic QA\\ngeneration\\n○\\nEvaluating Multi-Modal RAG\\n●\\nUnderstand the tools and techniques to automatically generate RAG evaluation\\ndata\\n○\\nThe Tech Buﬀet #16: Quickly Evaluate your RAG Without Manually\\nLabeling Test Data\\n○\\nGenerating a Synthetic Dataset for RAG\\n○\\n●\\nLearn key packages to planning, building, testing, monitoring, and deploying\\nenterprise grade RAG system\\n○\\nIterate on LLMs faster: Measure LLM quality and catch regressions\\n○\\nBuilding RAG-based LLM Applications for Production\\n○\\nARES: An Automated Evaluation Framework for Retrieval-Augmented\\nGeneration Systems\\n●\\nUnderstand the end-to-end technology stack of RAG systems\\n○\\nEnd-to-End LLMOps Platform\\n○\\nAn Enterprise-Grade Reference Architecture for the Production\\nDeployment of LLMs Using the RAG Pattern on Azure OpenAI\\nTask 2: Design and Develop the Prompt Generation System\\n●\\nUsers can input a description of their objective or task and specify a few scenarios\\nalong with their expected outputs.\\n●\\nWrite or adopt sophisticated algorithms, you generate multiple prompt options\\nbased on the provided information.\\n●\\nThis automated prompt generation process saves time and provides a diverse\\nrange of alternatives to consider. But add an evaluation metrics that check\\nwhether the generated prompt candidate aligns with the input description.\\nTask 3: Implement Evaluation Data Generation and Evaluation\\nTo further enhance the prompt generation process, incorporate automatic Evaluation\\nData Generation.\\n●\\nBy analysing the description provided by the user, create a set of test cases that\\nserve as evaluation benchmarks for the prompt candidates.\\n●\\nThese test cases simulate various scenarios, enabling users to observe how each\\nprompt performs in diﬀerent contexts.\\n●\\nThe generated test cases serve as a starting point, sparking creativity and\\ninspiring additional test cases for comprehensive evaluation.\\nTask 4 : Prompt Testing and Ranking\\nGoals\\nComprehensive Evaluation: Provide a robust system that uses various methodologies\\nfor a thorough assessment of prompts.\\nCustomizable and User-Centric: Allow users to choose or customize their preferred\\nevaluation methods.\\nDynamic and Adaptive: Ensure the system remains ﬂexible and adaptive, capable of\\nincorporating new ranking methodologies as they emerge.\\nPrimary Methods\\n●\\nMonte Carlo Matchmaking: This method is used to select and match diﬀerent\\nprompt candidates against each other. The Monte Carlo method, known for its\\napplications in problem-solving and decision-making processes, helps in\\noptimizing the information gained from each prompt battle. By simulating various\\nmatchups, it allows the system to test the eﬀectiveness of each prompt in\\ndiﬀerent scenarios.\\n●\\nELO Rating System: This system, which is commonly used in chess and other\\ncompetitive games, rates the prompts based on their performance in the battles.\\nEach prompt candidate is assigned a rating that reﬂects its success in previous\\nmatchups. The system takes into account not just the number of wins but also the\\nstrength of the opponents each prompt has defeated. This rating helps in\\nobjectively ranking the prompts based on their eﬀectiveness.\\nAdditional Ranking and Matching Mechanisms\\n●\\nTrueSkill Rating System: Ideal for scenarios involving multiple competitors,\\nadjusting ratings based on not just wins and losses but also the uncertainty in\\nperformance.\\n●\\nGlicko Rating System: Similar to ELO but with added ﬂexibility, accounting for the\\nvolatility in a player\\'s (or prompt’s) performance and the reliability of their rating.\\n●\\nBayesian Rating Systems: Applies Bayesian inference for a probabilistic approach\\nto rating, considering uncertainties and contextual variations in prompt\\nperformance.\\n●\\nPairwise Comparison Methods: Involves direct comparisons between pairs of\\nprompts, potentially integrating user preferences or expert evaluations into the\\nranking process.\\n●\\nCategorical Ranking: Instead of a numerical rating, prompts are categorized\\nbased on performance criteria like creativity, relevance, etc., for more qualitative\\nassessments.\\n●\\nAdaptive Ranking Algorithms: Algorithms that learn and adjust over time,\\nconsidering historical performance data and evolving user preferences or\\nrequirements.\\n●\\nSemantic Similarity Matching: Using NLP techniques to match prompts based on\\nsemantic content, ideal for understanding nuanced diﬀerences in prompt\\neﬀectiveness.\\nYou should adopt an innovative approach to prompt evaluation by utilizing Monte Carlo\\nmatchmaking and ELO rating systems, or any alternative method to match and rank.\\nTask 5: User Interface Development\\nDevelop a user-friendly interface for interacting with the prompt engineering system.\\n●\\nUI Design: Plan and design a user interface that allows users to easily input data,\\nreceive prompts, and view evaluation results.\\n●\\nUI Implementation: Develop and integrate the user interface with the backend\\nprompt engineering system.\\nTask 6: System Integration and Testing\\n●\\nIntegrate all components of the system and conduct comprehensive testing.\\n●\\nIntegrate the prompt generation, Evaluation Data Generation, evaluation, and user\\ninterface components.\\n●\\nTest the entire system for functionality, usability, and performance. Reﬁne based\\non feedback and test results.\\nTutorials Schedule\\nIn the following, the colour purple indicates morning sessions, and blue indicates\\nafternoon sessions.\\nMonday: Understanding Prompt engineering\\nHere the trainees will understand the week’s challenge.\\n●\\nIntroduction to Week Challenge (Yabebal)\\n●\\nIntroduction and challenge to prompt engineering (Fikerte)\\nKey Performance Indicators:\\n●\\nUnderstanding week’s challenge\\n●\\nUnderstanding the prompt engineering\\n●\\nAbility to reuse previous knowledge\\nTuesday\\n●\\nRAG components (Rehmet)\\n●\\nTechniques to improving R (Retrievers) in RAG (Emitnan)\\nKey Performance Indicators:\\n●\\nUnderstanding Prompt ranking\\n●\\nUnderstanding prompt matching\\n●\\nAbility to reuse previous knowledge\\nWednesday\\n●\\nRAG Evaluation Data Generation (Abel)\\n●\\nUnderstanding of prompt matching and ranking (Mahlet)\\nThursday\\n●\\nRAG evaluation metrics (Emitnan)\\n●\\nRAGObs - DevObs of RAG development and production deployment\\nDeliverables\\nNOTE: Document should be a PDF stored in google drive or published blog link. DO NOT\\nSUBMIT A LINK as PDF! If you want to submit pdf document, it should be the content of\\nyour report not a link.\\nInterim Submission - Wednesday 8pm UTC\\n●\\nLink to your code in GitHub\\n○\\nRepository where you will be using to complete the tasks in this week\\'s\\nchallenge. A minimum requirement is that you have a well structured\\nrepository and some coding progress is made.\\n●\\nA review report of your reading and understanding of Task 1 and any progress you\\nmade in other tasks.\\nFeedback\\nYou may not receive detailed comments on your interim submission, but will receive a\\ngrade.\\nFinal Submission - Saturday 8pm UTC\\n●\\nLink to your code in GitHub\\n○\\nComplete work for Automatic prompt generation\\n○\\nComplete work for Automatic evaluation\\n○\\nComplete work for Evaluation Data Generation\\n●\\nA blog post entry (which you can submit for example to Medium publishing) or a\\npdf report. .\\nFeedback\\nYou will receive comments/feedback in addition to a grade.\\nReferences\\n●\\nMeistrari didn’t see a good solution for prompt engineering, so it’s building one\\n●\\nAutoPrompt: Eliciting Knowledge from Language Models with Automatically\\nGenerated Prompts\\n●\\nLarge Language Models Are Human-Level Prompt Engineers\\n●\\nPrompt Engineering\\n●\\nHow to Create a Monte Carlo Simulation using Python\\n●\\nMonte Carlo Method Explained\\n●\\nWhat is Monte Carlo Simulation? How does it work?\\n●\\nElo Rating Algorithm\\n●\\nElo algorithm implementation in Python\\n●\\nTrueSkillTM: A Bayesian skill rating system\\nCompanies doing something similar to this project\\n●\\nAI Prompt Generator (promptlygenerated.com)\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import fitz  # PyMuPDF\n",
    "\n",
    "# def extract_text_from_pdf(pdf_path):\n",
    "#     text = \"\"\n",
    "#     doc = fitz.open(pdf_path)\n",
    "#     for page_num in range(doc.page_count):\n",
    "#         page = doc.load_page(page_num)\n",
    "#         text += page.get_text(\"text\")\n",
    "#     doc.close()\n",
    "#     return text\n",
    "\n",
    "# pdf_path = \"../Dataset/10 Academy Cohort A - Weekly Challenge_ Week - 6.pdf\"\n",
    "# text_from_pdf = extract_text_from_pdf(pdf_path)\n",
    "# text_from_pdf\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we connect to the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "index_name = 'llama-2-rag'\n",
    "\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    pinecone.create_index(\n",
    "        index_name,\n",
    "        dimension=1536,\n",
    "        metric='cosine'\n",
    "    )\n",
    "    # wait for index to finish initialization\n",
    "    while not pinecone.describe_index(index_name).status['ready']:\n",
    "        time.sleep(1)\n",
    "\n",
    "index = pinecone.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.04838,\n",
       " 'namespaces': {'': {'vector_count': 4838}},\n",
       " 'total_vector_count': 4838}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our index is now ready but it's empty. It is a vector index, so it needs vectors. As mentioned, to create these vector embeddings we will OpenAI's `text-embedding-ada-002` model — we can access it via LangChain like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elias/miniconda/envs/rag1/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embed_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this model we can create embeddings like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1536)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [\n",
    "    'this is the first chunk of text',\n",
    "    'then another second chunk of text is here'\n",
    "]\n",
    "\n",
    "res = embed_model.embed_documents(texts)\n",
    "len(res), len(res[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we get two (aligning to our two chunks of text) 1536-dimensional embeddings.\n",
    "\n",
    "We're now ready to embed and index all our our data! We do this by looping through our dataset and embedding and inserting everything in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [06:08<00:00,  7.51s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm  # for progress bar\n",
    "\n",
    "data = dataset.to_pandas()  # this makes it easier to iterate over the dataset\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "for i in tqdm(range(0, len(data), batch_size)):\n",
    "    i_end = min(len(data), i+batch_size)\n",
    "    # get batch of data\n",
    "    batch = data.iloc[i:i_end]\n",
    "    # generate unique ids for each chunk\n",
    "    ids = [f\"{x['doi']}-{x['chunk-id']}\" for i, x in batch.iterrows()]\n",
    "    # get text to embed\n",
    "    texts = [x['chunk'] for _, x in batch.iterrows()]\n",
    "    # embed text\n",
    "    embeds = embed_model.embed_documents(texts)\n",
    "    # get metadata to store in Pinecone\n",
    "    metadata = [\n",
    "        {'text': x['chunk'],\n",
    "         'source': x['source'],\n",
    "         'title': x['title']} for i, x in batch.iterrows()\n",
    "    ]\n",
    "    # add to Pinecone\n",
    "    index.upsert(vectors=zip(ids, embeds, metadata))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that the vector index has been populated using `describe_index_stats` like before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.04838,\n",
       " 'namespaces': {'': {'vector_count': 4838}},\n",
       " 'total_vector_count': 4838}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieval Augmented Generation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've built a fully-fledged knowledge base. Now it's time to connect that knowledge base to our chatbot. To do that we'll be diving back into LangChain and reusing our template prompt from earlier."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use LangChain here we need to load the LangChain abstraction for a vector index, called a `vectorstore`. We pass in our vector `index` to initialize the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elias/miniconda/envs/rag1/lib/python3.11/site-packages/langchain_community/vectorstores/pinecone.py:75: UserWarning: Passing in `embedding` as a Callable is deprecated. Please pass in an Embeddings object instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "text_field = \"text\"  # the metadata field that contains our text\n",
    "\n",
    "# initialize the vector store object\n",
    "vectorstore = Pinecone(\n",
    "    index, embed_model.embed_query, text_field\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User input: \"What is so special about Llama 2?\"\n"
     ]
    }
   ],
   "source": [
    "from user_input import get_user_input\n",
    "\n",
    "# Get user input\n",
    "user_input = get_user_input()\n",
    "print(\"User input:\", user_input)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this `vectorstore` we can already query the index and see if we have any relevant information given our question about Llama 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"What is so special about Llama 2?\"\n"
     ]
    }
   ],
   "source": [
    "query = user_input\n",
    "\n",
    "print(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\\nRoss Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\\nAngela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\\nSergey Edunov Thomas Scialom\\x03\\nGenAI, Meta\\nAbstract\\nIn this work, we develop and release Llama 2, a collection of pretrained and ﬁne-tuned\\nlarge language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\\nOur ﬁne-tuned LLMs, called L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , are optimized for dialogue use cases. Our\\nmodels outperform open-source chat models on most benchmarks we tested, and based on\\nourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosedsource models. We provide a detailed description of our approach to ﬁne-tuning and safety', metadata={'source': 'http://arxiv.org/pdf/2307.09288', 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models'}),\n",
       " Document(page_content='asChatGPT,BARD,andClaude. TheseclosedproductLLMsareheavilyﬁne-tunedtoalignwithhuman\\npreferences, which greatly enhances their usability and safety. This step can require signiﬁcant costs in\\ncomputeandhumanannotation,andisoftennottransparentoreasilyreproducible,limitingprogresswithin\\nthe community to advance AI alignment research.\\nIn this work, we develop and release Llama 2, a family of pretrained and ﬁne-tuned LLMs, L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle and\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc models generally perform better than existing open-source models. They also appear to\\nbe on par with some of the closed-source models, at least on the human evaluations we performed (see', metadata={'source': 'http://arxiv.org/pdf/2307.09288', 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models'}),\n",
       " Document(page_content='A2qXzbHspgrZJIa0/PcFIOcahRM8lmlnxmeUjahI96zVNGYmyBfLDsjF1YZkijR9ikkC/V3IqexMdM4tJMxxbFZ9ebif14vw+gmyIVKM+SKLT+KMkkwIfPLyVBozlBOLaFMC7srYWOqKUPbT8WW4K2evE7aV3XPrXsP17XmbVFHGc7gHC7BgwY04R5a4AMDAc/wCm+Ocl6cd+djOVpyiswp/IHz+QPvmo68</latexit>G1', metadata={'source': 'http://arxiv.org/pdf/1806.01261', 'title': 'Relational inductive biases, deep learning, and graph networks'})]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.similarity_search(query, k=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We return a lot of text here and it's not that clear what we need or what is relevant. Fortunately, our LLM will be able to parse this information much faster than us. All we need is to connect the output from our `vectorstore` to our `chat` chatbot. To do that we can use the same logic as we used earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_prompt(query: str, folder_path: str):\n",
    "    # get top 3 results from knowledge base\n",
    "    results = vectorstore.similarity_search(query, k=3)\n",
    "    # get the text from the results\n",
    "    source_knowledge = \"\\n\".join([x.page_content for x in results])\n",
    "    # feed into an augmented prompt\n",
    "    augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
    "\n",
    "    Contexts:\n",
    "    {source_knowledge}\n",
    "\n",
    "    Query: {query}\"\"\"\n",
    "    \n",
    "    # save the source knowledge to a file in the specified folder\n",
    "    file_path = os.path.join(folder_path, 'context.txt')\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(source_knowledge)\n",
    "    \n",
    "    return augmented_prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the contexts below, answer the query.\n",
      "\n",
      "    Contexts:\n",
      "    Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\n",
      "Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\n",
      "Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\n",
      "Sergey Edunov Thomas Scialom\u0003\n",
      "GenAI, Meta\n",
      "Abstract\n",
      "In this work, we develop and release Llama 2, a collection of pretrained and ﬁne-tuned\n",
      "large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\n",
      "Our ﬁne-tuned LLMs, called L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , are optimized for dialogue use cases. Our\n",
      "models outperform open-source chat models on most benchmarks we tested, and based on\n",
      "ourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosedsource models. We provide a detailed description of our approach to ﬁne-tuning and safety\n",
      "asChatGPT,BARD,andClaude. TheseclosedproductLLMsareheavilyﬁne-tunedtoalignwithhuman\n",
      "preferences, which greatly enhances their usability and safety. This step can require signiﬁcant costs in\n",
      "computeandhumanannotation,andisoftennottransparentoreasilyreproducible,limitingprogresswithin\n",
      "the community to advance AI alignment research.\n",
      "In this work, we develop and release Llama 2, a family of pretrained and ﬁne-tuned LLMs, L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle and\n",
      "L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\n",
      "L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc models generally perform better than existing open-source models. They also appear to\n",
      "be on par with some of the closed-source models, at least on the human evaluations we performed (see\n",
      "A2qXzbHspgrZJIa0/PcFIOcahRM8lmlnxmeUjahI96zVNGYmyBfLDsjF1YZkijR9ikkC/V3IqexMdM4tJMxxbFZ9ebif14vw+gmyIVKM+SKLT+KMkkwIfPLyVBozlBOLaFMC7srYWOqKUPbT8WW4K2evE7aV3XPrXsP17XmbVFHGc7gHC7BgwY04R5a4AMDAc/wCm+Ocl6cd+djOVpyiswp/IHz+QPvmo68</latexit>G1\n",
      "\n",
      "    Query: \"What is so special about Llama 2?\"\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"../prompts\"\n",
    "print(augment_prompt(query, folder_path))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this we produce an augmented prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the contexts below, answer the query.\n",
      "\n",
      "    Contexts:\n",
      "    Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\n",
      "Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\n",
      "Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\n",
      "Sergey Edunov Thomas Scialom\u0003\n",
      "GenAI, Meta\n",
      "Abstract\n",
      "In this work, we develop and release Llama 2, a collection of pretrained and ﬁne-tuned\n",
      "large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\n",
      "Our ﬁne-tuned LLMs, called L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , are optimized for dialogue use cases. Our\n",
      "models outperform open-source chat models on most benchmarks we tested, and based on\n",
      "ourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosedsource models. We provide a detailed description of our approach to ﬁne-tuning and safety\n",
      "asChatGPT,BARD,andClaude. TheseclosedproductLLMsareheavilyﬁne-tunedtoalignwithhuman\n",
      "preferences, which greatly enhances their usability and safety. This step can require signiﬁcant costs in\n",
      "computeandhumanannotation,andisoftennottransparentoreasilyreproducible,limitingprogresswithin\n",
      "the community to advance AI alignment research.\n",
      "In this work, we develop and release Llama 2, a family of pretrained and ﬁne-tuned LLMs, L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle and\n",
      "L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\n",
      "L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc models generally perform better than existing open-source models. They also appear to\n",
      "be on par with some of the closed-source models, at least on the human evaluations we performed (see\n",
      "A2qXzbHspgrZJIa0/PcFIOcahRM8lmlnxmeUjahI96zVNGYmyBfLDsjF1YZkijR9ikkC/V3IqexMdM4tJMxxbFZ9ebif14vw+gmyIVKM+SKLT+KMkkwIfPLyVBozlBOLaFMC7srYWOqKUPbT8WW4K2evE7aV3XPrXsP17XmbVFHGc7gHC7BgwY04R5a4AMDAc/wCm+Ocl6cd+djOVpyiswp/IHz+QPvmo68</latexit>G1\n",
      "\n",
      "    Query: \"What is so special about Llama 2?\"\n"
     ]
    }
   ],
   "source": [
    "print(augment_prompt(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is still a lot of text here, so let's pass it onto our chat model to see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 2 is a collection of pretrained and fine-tuned large language models (LLMs) developed in the mentioned work. These LLMs range in scale from 7 billion to 70 billion parameters. They are specifically optimized for dialogue use cases.\n",
      "\n",
      "The special aspect of Llama 2 is that these models outperform open-source chat models on most benchmarks that were tested. Additionally, based on humane evaluations for helpfulness and safety, Llama 2 models have the potential to be a suitable substitute for closed-source models. This is significant because closed-source models are heavily fine-tuned to align with human preferences, enhancing usability and safety. However, fine-tuning closed-source models can be costly in terms of compute resources and human annotation, and it may not always be transparent or easily reproducible, limiting progress within the AI alignment research community.\n",
      "\n",
      "In summary, Llama 2 stands out due to its collection of pretrained and fine-tuned LLMs optimized for dialogue use cases, their superior performance compared to open-source chat models, and their potential as an alternative to closed-source models in terms of helpfulness and safety.\n"
     ]
    }
   ],
   "source": [
    "# create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=augment_prompt(query)\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "res = chat(messages)\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can continue with more Llama 2 questions. Let's try _without_ RAG first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I apologize for the confusion, but based on the given contexts, there is no specific information provided about safety measures used in the development of Llama 2. The provided contexts primarily focus on the concept of chains, LangChain framework, and the general structure and components of an LLMChain. It does not mention any specific safety measures related to the development process of Llama 2.\n"
     ]
    }
   ],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=\"what safety measures were used in the development of llama 2?\"\n",
    ")\n",
    "\n",
    "res = chat(messages + [prompt])\n",
    "print(res.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chatbot is able to respond about Llama 2 thanks to it's conversational history stored in `messages`. However, it doesn't know anything about the safety measures themselves as we have not provided it with that information via the RAG pipeline. Let's try again but with RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the contexts below, answer the query.\n",
      "\n",
      "    Contexts:\n",
      "    /nqDRRxgkpLODamG4WZjXOsLSOcTss9Z2iGyRgPaddTiQU1cT4/d4rOvTJAqdK+pEVz9fdEjoUxE5H4ToHtyCx7M/E/r+tsehPnTGbOUkkWi1LHkVVo9jsaME2J5RNPMNHM34rICGtMrE+o7EOIll9eJa3LWhTWooerav22iKMEp3AGFxDBNdThHhrQBAJjeIZXeAuy4CV4Dz4WrWtBMXMCfxB8/gCVCY+3</latexit>\n",
      "Je/eoOYppIpSwUxput7iQ0yoi2ngk1LvdSwhNAxGbKuo4pIZoJsfuoUnzllgKNYu1IWz9XfExmRxkxk6DolsSOz7M3E/7xuaqPrIOMqSS1TdLEoSgW2MZ79jQdcM2rFxBFCNXe3YjoimlDr0im5EPzll1dJ67zmezX//rJSr+ZxFOEETqEKPlxBHe6gAU2gMIRneIU3JNALekcfi9YCymeO4Q/Q5w/Ca41b</latexit>\n",
      "A2qXzbHspgrZJIa0/PcFIOcahRM8lmlnxmeUjahI96zVNGYmyBfLDsjF1YZkijR9ikkC/V3IqexMdM4tJMxxbFZ9ebif14vw+gmyIVKM+SKLT+KMkkwIfPLyVBozlBOLaFMC7srYWOqKUPbT8WW4K2evE7aV3XPrXsP17XmbVFHGc7gHC7BgwY04R5a4AMDAc/wCm+Ocl6cd+djOVpyiswp/IHz+QPvmo68</latexit>G1\n",
      "\n",
      "    Query: \n",
      "In the development of Llama 2, several safety measures were employed to increase the safety of the models. These measures include safety-specific data annotation and tuning, red-teaming, and iterative evaluations.\n",
      "\n",
      "Safety-specific data annotation and tuning: The development process involved annotating and tuning the models with a focus on safety. This likely means that specific guidelines and criteria were used to ensure the models' responses align with safety standards and avoid harmful or inappropriate content.\n",
      "\n",
      "Red-teaming: Red-teaming refers to the practice of having external teams or individuals simulate adversarial scenarios to identify potential risks or vulnerabilities. By conducting red-teaming exercises, the developers aimed to assess and address any potential safety issues in the Llama 2 models.\n",
      "\n",
      "Iterative evaluations: The development process likely included iterative evaluations, which involve continuously assessing and refining the models' performance and safety. This iterative approach allows for ongoing improvements and mitigations of safety concerns.\n",
      "\n",
      "These safety measures were implemented to enhance the safety of Llama 2 models and ensure responsible development practices. The details of the specific safety measures and techniques used may not be explicitly mentioned in the provided context, but the general approach of prioritizing safety and conducting evaluations and refinements is highlighted.\n"
     ]
    }
   ],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=augment_prompt(\n",
    "        \"what safety measures were used in the development of llama 2?\"\n",
    "    )\n",
    ")\n",
    "\n",
    "res = chat(messages + [prompt])\n",
    "print(augment_prompt(query))\n",
    "print(res.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a much more informed response that includes several items missing in the previous non-RAG response, such as \"red-teaming\", \"iterative evaluations\", and the intention of the researchers to share this research to help \"improve their safety, promoting responsible development in the field\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
