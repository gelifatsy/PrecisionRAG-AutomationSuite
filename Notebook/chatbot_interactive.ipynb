{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/learn/generation/langchain/rag-chatbot.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/learn/generation/langchain/rag-chatbot.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building RAG Chatbots with LangChain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we'll work on building an AI chatbot from start-to-finish. We will be using LangChain, OpenAI, and Pinecone vector DB, to build a chatbot capable of learning from the external world using **R**etrieval **A**ugmented **G**eneration (RAG).\n",
    "\n",
    "We will be using a dataset sourced from the Llama 2 ArXiv paper and other related papers to help our chatbot answer questions about the latest and greatest in the world of GenAI.\n",
    "\n",
    "By the end of the example we'll have a functioning chatbot and RAG pipeline that can hold a conversation and provide informative responses based on a knowledge base.\n",
    "\n",
    "### Before you begin\n",
    "\n",
    "You'll need to get an [OpenAI API key](https://platform.openai.com/account/api-keys) and [Pinecone API key](https://app.pinecone.io)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start building our chatbot, we need to install some Python libraries. Here's a brief overview of what each library does:\n",
    "\n",
    "- **langchain**: This is a library for GenAI. We'll use it to chain together different language models and components for our chatbot.\n",
    "- **openai**: This is the official OpenAI Python client. We'll use it to interact with the OpenAI API and generate responses for our chatbot.\n",
    "- **datasets**: This library provides a vast array of datasets for machine learning. We'll use it to load our knowledge base for the chatbot.\n",
    "- **pinecone-client**: This is the official Pinecone Python client. We'll use it to interact with the Pinecone API and store our chatbot's knowledge base in a vector database.\n",
    "\n",
    "You can install these libraries using pip like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "flagembedding 1.1.8 requires transformers==4.34.0, but you have transformers 4.36.2 which is incompatible.\n",
      "ragas 0.0.22 requires openai>1, but you have openai 0.28.0 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip install -qU \\\n",
    "#     langchain==0.0.292 \\\n",
    "#     openai==0.28.0 \\\n",
    "#     datasets==2.10.1 \\\n",
    "#     pinecone-client==2.2.4 \\\n",
    "#     tiktoken==0.5.1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Chatbot (no RAG)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be relying heavily on the LangChain library to bring together the different components needed for our chatbot. To begin, we'll create a simple chatbot without any retrieval augmentation. We do this by initializing a `ChatOpenAI` object. For this we do need an [OpenAI API key](https://platform.openai.com/account/api-keys)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "OPENAI_API_KEY=os.getenv(\"OPENAI_API_KEY\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elias/miniconda/envs/rag1/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "OPENAI_API_KEY= os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") \n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    model='gpt-3.5-turbo'\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chats with OpenAI's `gpt-3.5-turbo` and `gpt-4` chat models are typically structured (in plain text) like this:\n",
    "\n",
    "```\n",
    "System: You are a helpful assistant.\n",
    "\n",
    "User: Hi AI, how are you today?\n",
    "\n",
    "Assistant: I'm great thank you. How can I help you?\n",
    "\n",
    "User: I'd like to understand string theory.\n",
    "\n",
    "Assistant:\n",
    "```\n",
    "\n",
    "The final `\"Assistant:\"` without a response is what would prompt the model to continue the conversation. In the official OpenAI `ChatCompletion` endpoint these would be passed to the model in a format like:\n",
    "\n",
    "```python\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hi AI, how are you today?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I'm great thank you. How can I help you?\"}\n",
    "    {\"role\": \"user\", \"content\": \"I'd like to understand string theory.\"}\n",
    "]\n",
    "```\n",
    "\n",
    "In LangChain there is a slightly different format. We use three _message_ objects like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Hi AI, how are you today?\"),\n",
    "    AIMessage(content=\"I'm great thank you. How can I help you?\"),\n",
    "    HumanMessage(content=\"I'd like to understand string theory.\")\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The format is very similar, we're just swapped the role of `\"user\"` for `HumanMessage`, and the role of `\"assistant\"` for `AIMessage`.\n",
    "\n",
    "We generate the next response from the AI by passing these messages to the `ChatOpenAI` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elias/miniconda/envs/rag1/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='String theory is a theoretical framework in physics that aims to describe the fundamental building blocks of the universe. It suggests that at the most fundamental level, everything in our universe is composed of tiny, vibrating strings rather than point-like particles. These strings can oscillate in different ways, producing different particles and forces that we observe in nature.\\n\\nHere are a few key concepts in string theory:\\n\\n1. Strings: Instead of particles being represented as point-like objects, string theory proposes that they are one-dimensional strings. These strings can be open or closed loops, and their vibrations give rise to the different particles and their properties.\\n\\n2. Extra Dimensions: String theory requires additional spatial dimensions beyond the familiar three dimensions of space (length, width, and height). These extra dimensions are typically compactified or curled up at extremely small scales, making them difficult to detect directly.\\n\\n3. Quantum Gravity: One of the main motivations behind string theory is to reconcile quantum mechanics (which describes the behavior of particles at small scales) with general relativity (which describes gravity on a large scale). String theory incorporates gravity into its framework and seeks to provide a consistent quantum theory of gravity.\\n\\n4. Multiple Universes: String theory suggests the possibility of the existence of multiple universes or a \"multiverse.\" These universes may have different physical laws and properties, and our observable universe could be just one of many.\\n\\nIt\\'s important to note that string theory is still a work in progress, and many aspects of it are still being actively researched. It has the potential to provide a unified description of all fundamental forces and particles, but experimental evidence is currently lacking.')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = chat(messages)\n",
    "res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In response we get another AI message object. We can print it more clearly like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, I can help you with that. String theory is a theoretical framework in physics that aims to describe the fundamental particles and forces of nature. It suggests that at the most fundamental level, everything in the universe is made up of tiny vibrating strings or loops of energy.\n",
      "\n",
      "According to string theory, these strings are incredibly small, with a length scale of about the Planck length (about 10^(-35) meters). The different vibrational patterns of these strings give rise to different particles, such as electrons, quarks, and photons.\n",
      "\n",
      "One of the key ideas in string theory is that it requires more than the usual four dimensions of space and time (three spatial dimensions and one time dimension) that we are familiar with. In fact, string theory suggests that there are additional hidden dimensions, usually referred to as \"extra dimensions,\" which are curled up or compactified at such small scales that we can't perceive them directly.\n",
      "\n",
      "These extra dimensions can take various forms, such as being shaped like a tiny loop or curled up in intricate ways. The number and shape of these extra dimensions have a significant impact on the properties of particles and the forces between them.\n",
      "\n",
      "String theory also proposes that there are different versions or \"string theories,\" which include various types of strings and interactions. These different versions are unified by a principle known as \"duality,\" which shows that seemingly different versions of string theory are mathematically equivalent descriptions of the same underlying physics.\n",
      "\n",
      "It's important to note that string theory is still a work in progress and remains highly theoretical. It has not yet been experimentally proven, and many aspects of the theory are still being explored and understood by physicists.\n",
      "\n",
      "I hope this gives you a basic understanding of string theory. Let me know if you have any more questions!\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because `res` is just another `AIMessage` object, we can append it to `messages`, add another `HumanMessage`, and generate the next response in the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physicists believe that string theory has the potential to provide a unified theory because it has the ability to incorporate all the fundamental forces of nature into a single framework. Currently, there are four known fundamental forces: gravity, electromagnetism, and the strong and weak nuclear forces.\n",
      "\n",
      "One of the challenges in physics has been to explain how these forces are related and to develop a theory that can describe them all consistently. String theory suggests that all particles and forces arise from the vibrations of tiny strings, which provides a potential framework for unification.\n",
      "\n",
      "In string theory, the different vibrational modes of the strings correspond to different particles and their interactions. As a result, it is possible to describe not only the particles of matter but also the particles that mediate the forces between them, such as photons for electromagnetic interactions and gravitons for gravity.\n",
      "\n",
      "Moreover, string theory also predicts the existence of additional particles, such as supersymmetric partners, which could help explain some of the discrepancies and open questions in our current understanding of particle physics.\n",
      "\n",
      "Additionally, string theory offers the possibility of incorporating gravity into a quantum mechanical framework, which is currently a major challenge in theoretical physics. By combining quantum mechanics and general relativity (the theory of gravity), string theory aims to provide a consistent description of the universe at both the microscopic and macroscopic scales.\n",
      "\n",
      "While string theory shows promise for unifying the fundamental forces, it is important to note that it is still a work in progress. Many aspects of the theory are still being explored, and there is ongoing research to develop a more complete and experimentally testable version of string theory.\n",
      "\n",
      "In summary, physicists believe that string theory has the potential to provide a unified theory by incorporating all the fundamental forces into a single framework and by offering a way to reconcile quantum mechanics and general relativity. However, further research and experimentation are needed to confirm and refine the theory.\n"
     ]
    }
   ],
   "source": [
    "# add latest AI response to messages\n",
    "messages.append(res)\n",
    "\n",
    "# now create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=\"Why do physicists believe it can produce a 'unified theory'?\"\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to chat-gpt\n",
    "res = chat(messages)\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Hallucinations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our chatbot, but as mentioned â€” the knowledge of LLMs can be limited. The reason for this is that LLMs learn all they know during training. An LLM essentially compresses the \"world\" as seen in the training data into the internal parameters of the model. We call this knowledge the _parametric knowledge_ of the model.\n",
    "\n",
    "By default, LLMs have no access to the external world.\n",
    "\n",
    "The result of this is very clear when we ask LLMs about more recent information, like about the new (and very popular) Llama 2 LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add latest AI response to messages\n",
    "messages.append(res)\n",
    "\n",
    "# now create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=\"What is so special about Llama 2?\"\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to OpenAI\n",
    "res = chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but I'm not aware of any specific significance or special meaning associated with \"Llama 2.\" It's possible that you may be referring to something specific, like a book, a movie, a game, or a particular reference in a context that I'm not aware of. Could you please provide more information or context so that I can better understand and assist you?\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our chatbot can no longer help us, it doesn't contain the information we need to answer the question. It was very clear from this answer that the LLM doesn't know the informaiton, but sometimes an LLM may respond like it _does_ know the answer â€” and this can be very hard to detect.\n",
    "\n",
    "OpenAI have since adjusted the behavior for this particular example as we can see below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add latest AI response to messages\n",
    "messages.append(res)\n",
    "\n",
    "# now create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=\"Can you tell me about the LLMChain in LangChain?\"\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to OpenAI\n",
    "res = chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I apologize, but I couldn't find any specific information about an \"LLMChain\" in relation to \"LangChain.\" It's possible that these terms are specific to a particular context or project that I'm not familiar with. If you can provide more details or clarify the context, I'll do my best to assist you further.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is another way of feeding knowledge into LLMs. It is called _source knowledge_ and it refers to any information fed into the LLM via the prompt. We can try that with the LLMChain question. We can take a description of this object from the LangChain documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "llmchain_information = [\n",
    "    \"A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format.\",\n",
    "    \"Chains is an incredibly generic concept which returns to a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case.\",\n",
    "    \"LangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data, (2) Be agentic: Allow a language model to interact with its environment. As such, the LangChain framework is designed with the objective in mind to enable those types of applications.\"\n",
    "]\n",
    "\n",
    "source_knowledge = \"\\n\".join(llmchain_information)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can feed this additional knowledge into our prompt with some instructions telling the LLM how we'd like it to use this information alongside our original query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Can you tell me about the LLMChain in LangChain?\"\n",
    "\n",
    "augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
    "\n",
    "Contexts:\n",
    "{source_knowledge}\n",
    "\n",
    "Query: {query}\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we feed this into our chatbot as we were before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=augmented_prompt\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to OpenAI\n",
    "res = chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The LLMChain is a specific type of chain within the LangChain framework, which is used for developing applications powered by language models. A chain in this context refers to a sequence of modular components combined in a specific way to achieve a particular purpose.\n",
      "\n",
      "The LLMChain, in particular, is the most common type of chain in LangChain. It consists of three main components: a PromptTemplate, a language model (either an LLM or a ChatModel), and an optional output parser. \n",
      "\n",
      "When using an LLMChain, multiple input variables are taken and formatted into a prompt using the PromptTemplate. This prompt is then passed to the language model, which generates an output based on the given input. Finally, the optional output parser can be used to parse and format the output of the language model into a final desired format.\n",
      "\n",
      "The LangChain framework aims to enable powerful and differentiated applications by not only integrating language models through an API but also by making them data-aware and agentic. Being data-aware allows the language model to connect with other sources of data, while being agentic enables the language model to interact with its environment.\n",
      "\n",
      "In summary, the LLMChain is a fundamental component of the LangChain framework, providing a way to structure and utilize language models within applications, with support for input formatting, language model processing, and output parsing.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quality of this answer is phenomenal. This is made possible thanks to the idea of augmented our query with external knowledge (source knowledge). There's just one problem â€” how do we get this information in the first place?\n",
    "\n",
    "We learned in the previous chapters about Pinecone and vector databases. Well, they can help us here too. But first, we'll need a dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, we will be importing our data. We will be using the Hugging Face Datasets library to load our data. Specifically, we will be using the `\"jamescalam/llama-2-arxiv-papers\"` dataset. This dataset contains a collection of ArXiv papers which will serve as the external knowledge base for our chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.76M/2.76M [00:01<00:00, 1.85MB/s]\n",
      "Generating train split: 2212 examples [00:00, 3045.48 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'text', 'source'],\n",
       "    num_rows: 2212\n",
       "})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"jamescalam/langchain-docs\",\n",
    "    split=\"train\"\n",
    ")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '9c04de564ed3-0',\n",
       " 'text': '.rst\\n.pdf\\nWelcome to LangChain\\n Contents \\nGetting Started\\nModules\\nUse Cases\\nReference Docs\\nLangChain Ecosystem\\nAdditional Resources\\nWelcome to LangChain#\\nLarge language models (LLMs) are emerging as a transformative technology, enabling\\ndevelopers to build applications that they previously could not.\\nBut using these LLMs in isolation is often not enough to\\ncreate a truly powerful app - the real power comes when you are able to\\ncombine them with other sources of computation or knowledge.\\nThis library is aimed at assisting in the development of those types of applications. Common examples of these types of applications include:\\nâ“ Question Answering over specific documents\\nDocumentation\\nEnd-to-end Example: Question Answering over Notion Database\\nðŸ’¬ Chatbots\\nDocumentation\\nEnd-to-end Example: Chat-LangChain\\nðŸ¤– Agents\\nDocumentation\\nEnd-to-end Example: GPT+WolframAlpha\\nGetting Started#\\nCheckout the below guide for a walkthrough of how to get started using LangChain to create an Language Model application.\\nGetting Started Documentation\\nModules#\\nThere are several main modules that LangChain provides support for.\\nFor each module we provide some examples to get started, how-to guides, reference docs, and conceptual guides.\\nThese modules are, in increasing order of complexity:\\nPrompts: This includes prompt management, prompt optimization, and prompt serialization.\\nLLMs: This includes a generic interface for all LLMs, and common utilities for working with LLMs.\\nDocument Loaders: This includes a standard interface for loading documents, as well as specific integrations to all types of text data sources.\\nUtils: Language models are often more powerful when interacting with other sources of knowledge or computation. This can include Python REPLs, embeddings, search engines, and more. LangChain provides a large collection of common utils to use in your application.',\n",
       " 'source': 'https://langchain.readthedocs.io/en/latest/index.html'}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Overview\n",
    "\n",
    "The dataset we are using is sourced from the Llama 2 ArXiv papers. It is a collection of academic papers from ArXiv, a repository of electronic preprints approved for publication after moderation. Each entry in the dataset represents a \"chunk\" of text from these papers.\n",
    "\n",
    "Because most **L**arge **L**anguage **M**odels (LLMs) only contain knowledge of the world as it was during training, they cannot answer our questions about Llama 2 â€” at least not without this data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Building the Knowledge Base"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a dataset that can serve as our chatbot knowledge base. Our next task is to transform that dataset into the knowledge base that our chatbot can use. To do this we must use an embedding model and vector database.\n",
    "\n",
    "We begin by initializing our connection to Pinecone, this requires a [free API key](https://app.pinecone.io)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pinecone\n",
    "\n",
    "# get API key from app.pinecone.io and environment from console\n",
    "pinecone.init(\n",
    "    api_key=os.environ.get('PINECONE_API_KEY') or 'PINECONE_API_KEY',\n",
    "    environment=os.environ.get('PINECONE_ENVIRONMENT') or 'gcp-starter'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we initialize the index. We will be using OpenAI's `text-embedding-ada-002` model for creating the embeddings, so we set the `dimension` to `1536`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import fitz  # PyMuPDF\n",
    "\n",
    "# def extract_text_from_pdf(pdf_path):\n",
    "#     text = \"\"\n",
    "#     doc = fitz.open(pdf_path)\n",
    "#     for page_num in range(doc.page_count):\n",
    "#         page = doc.load_page(page_num)\n",
    "#         text += page.get_text(\"text\")\n",
    "#     doc.close()\n",
    "#     return text\n",
    "\n",
    "# pdf_path = \"../Dataset/10 Academy Cohort A - Weekly Challenge_ Week - 6.pdf\"\n",
    "# data = extract_text_from_pdf(pdf_path)\n",
    "# print(data)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we connect to the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "index_name = \"langchain\"\n",
    "\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    pinecone.create_index(\n",
    "        index_name,\n",
    "        dimension=1536,\n",
    "        metric='cosine'\n",
    "    )\n",
    "    # wait for index to finish initialization\n",
    "    while not pinecone.describe_index(index_name).status['ready']:\n",
    "        time.sleep(1)\n",
    "\n",
    "index = pinecone.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {},\n",
       " 'total_vector_count': 0}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our index is now ready but it's empty. It is a vector index, so it needs vectors. As mentioned, to create these vector embeddings we will OpenAI's `text-embedding-ada-002` model â€” we can access it via LangChain like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embed_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this model we can create embeddings like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1536)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [\n",
    "    'this is the first chunk of text',\n",
    "    'then another second chunk of text is here'\n",
    "]\n",
    "\n",
    "res = embed_model.embed_documents(texts)\n",
    "len(res), len(res[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we get two (aligning to our two chunks of text) 1536-dimensional embeddings.\n",
    "\n",
    "We're now ready to embed and index all our our data! We do this by looping through our dataset and embedding and inserting everything in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [03:05<00:00,  8.05s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm  # for progress bar\n",
    "\n",
    "data = dataset.to_pandas()  # this makes it easier to iterate over the dataset\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "for i in tqdm(range(0, len(data), batch_size)):\n",
    "    i_end = min(len(data), i+batch_size)\n",
    "    # get batch of data\n",
    "    batch = data.iloc[i:i_end]\n",
    "    # generate unique ids for each chunk\n",
    "    ids = [f\"{x['id']}\" for _, x in batch.iterrows()]\n",
    "\n",
    "    # get text to embed\n",
    "    texts = [x['text'] for _, x in batch.iterrows()]\n",
    "    # embed text\n",
    "    embeds = embed_model.embed_documents(texts)\n",
    "    # get metadata to store in Pinecone\n",
    "    metadata = [\n",
    "        {'text': x['text'],\n",
    "         'source': x['source']\n",
    "         } for i, x in batch.iterrows()\n",
    "    ]\n",
    "    # add to Pinecone\n",
    "    index.upsert(vectors=zip(ids, embeds, metadata))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that the vector index has been populated using `describe_index_stats` like before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.02212,\n",
       " 'namespaces': {'': {'vector_count': 2212}},\n",
       " 'total_vector_count': 2212}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index_name = '10-academy'\n",
    "\n",
    "# # Delete the entire index\n",
    "# pinecone.delete_index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The database is empty.\n"
     ]
    }
   ],
   "source": [
    "# indexes = pinecone.list_indexes()\n",
    "\n",
    "# # Check if the list of indexes is empty\n",
    "# if not indexes:\n",
    "#     print(\"The database is empty.\")\n",
    "# else:\n",
    "#     print(\"The database is not empty.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieval Augmented Generation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've built a fully-fledged knowledge base. Now it's time to connect that knowledge base to our chatbot. To do that we'll be diving back into LangChain and reusing our template prompt from earlier."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use LangChain here we need to load the LangChain abstraction for a vector index, called a `vectorstore`. We pass in our vector `index` to initialize the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elias/miniconda/envs/rag1/lib/python3.11/site-packages/langchain_community/vectorstores/pinecone.py:75: UserWarning: Passing in `embedding` as a Callable is deprecated. Please pass in an Embeddings object instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "text_field = \"text\"  # the metadata field that contains our text\n",
    "\n",
    "# initialize the vector store object\n",
    "vectorstore = Pinecone(\n",
    "    index, embed_model.embed_query, text_field\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_input():\n",
    "    \"\"\"\n",
    "    Get user input from the console and return it as a string.\n",
    "    \"\"\"\n",
    "    user_input = input(\"Enter your query: \")\n",
    "    return str(user_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_input():\n",
    "    \"\"\"\n",
    "    Get user input from the console and return it as a string.\n",
    "    \"\"\"\n",
    "    user_input = input(\"Enter your query: \")\n",
    "    return str(user_input)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this `vectorstore` we can already query the index and see if we have any relevant information given our question about Llama 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is langchain\n"
     ]
    }
   ],
   "source": [
    "query = get_user_input()\n",
    "\n",
    "print(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Chatbots\\\\n\\\\nDocumentation\\\\n\\\\nEnd-to-end Example: Chat-LangChain\\\\n\\\\nÃ°\\\\x9fÂ¤\\\\x96 Agents\\\\n\\\\nDocumentation\\\\n\\\\nEnd-to-end Example: GPT+WolframAlpha\\\\n\\\\nÃ°\\\\x9fâ€œ\\\\x96 Documentation\\\\n\\\\nPlease see here for full documentation on:\\\\n\\\\nGetting started (installation, setting up the environment, simple examples)\\\\n\\\\nHow-To examples (demos, integrations, helper functions)\\\\n\\\\nReference (full API docs)\\\\n  Resources (high-level explanation of core concepts)\\\\n\\\\nÃ°\\\\x9f\\\\x9a\\\\x80 What can this help with?\\\\n\\\\nThere are six main areas that LangChain is designed to help with.\\\\nThese are, in increasing order of complexity:\\\\n\\\\nÃ°\\\\x9fâ€œ\\\\x83 LLMs and Prompts:\\\\n\\\\nThis includes prompt management, prompt optimization, generic interface for all LLMs, and common utilities for working with LLMs.\\\\n\\\\nÃ°\\\\x9fâ€\\\\x97 Chains:\\\\n\\\\nChains go beyond just a single LLM call, and are sequences of calls (whether to an LLM or a different utility). LangChain provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications.\\\\n\\\\nÃ°\\\\x9fâ€œ\\\\x9a', metadata={'source': 'https://langchain.readthedocs.io/en/latest/modules/document_loaders/examples/markdown.html'}),\n",
       " Document(page_content='chains for common applications.\\\\n\\\\nÃ°\\\\x9fâ€œ\\\\x9a Data Augmented Generation:\\\\n\\\\nData Augmented Generation involves specific types of chains that first interact with an external datasource to fetch data to use in the generation step. Examples of this include summarization of long pieces of text and question/answering over specific data sources.\\\\n\\\\nÃ°\\\\x9fÂ¤\\\\x96 Agents:\\\\n\\\\nAgents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done. LangChain provides a standard interface for agents, a selection of agents to choose from, and examples of end to end agents.\\\\n\\\\nÃ°\\\\x9fÂ§\\\\xa0 Memory:\\\\n\\\\nMemory is the concept of persisting state between calls of a chain/agent. LangChain provides a standard interface for memory, a collection of memory implementations, and examples of chains/agents that use memory.\\\\n\\\\nÃ°\\\\x9fÂ§\\\\x90 Evaluation:\\\\n\\\\n[BETA] Generative models are notoriously hard to evaluate with traditional metrics. One new way of evaluating them is using language models themselves to do the evaluation.', metadata={'source': 'https://langchain.readthedocs.io/en/latest/modules/document_loaders/examples/markdown.html'}),\n",
       " Document(page_content='[Document(page_content=\"Ã°\\\\x9fÂ¦\\\\x9cÃ¯Â¸\\\\x8fÃ°\\\\x9fâ€\\\\x97 LangChain\\\\n\\\\nÃ¢\\\\x9aÂ¡ Building applications with LLMs through composability Ã¢\\\\x9aÂ¡\\\\n\\\\nProduction Support: As you move your LangChains into production, we\\'d love to offer more comprehensive support.\\\\nPlease fill out this form and we\\'ll set up a dedicated support Slack channel.\\\\n\\\\nQuick Install\\\\n\\\\npip install langchain\\\\n\\\\nÃ°\\\\x9fÂ¤â€ What is this?\\\\n\\\\nLarge language models (LLMs) are emerging as a transformative technology, enabling\\\\ndevelopers to build applications that they previously could not.\\\\nBut using these LLMs in isolation is often not enough to\\\\ncreate a truly powerful app - the real power comes when you can combine them with other sources of computation or knowledge.\\\\n\\\\nThis library is aimed at assisting in the development of those types of applications. Common examples of these types of applications include:\\\\n\\\\nÃ¢\\\\x9dâ€œ Question Answering over specific documents\\\\n\\\\nDocumentation\\\\n\\\\nEnd-to-end Example: Question Answering over Notion Database\\\\n\\\\nÃ°\\\\x9fâ€™Â¬ Chatbots\\\\n\\\\nDocumentation\\\\n\\\\nEnd-to-end Example:', metadata={'source': 'https://langchain.readthedocs.io/en/latest/modules/document_loaders/examples/markdown.html'})]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.similarity_search(query, k=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We return a lot of text here and it's not that clear what we need or what is relevant. Fortunately, our LLM will be able to parse this information much faster than us. All we need is to connect the output from our `vectorstore` to our `chat` chatbot. To do that we can use the same logic as we used earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_prompt(folder_path, query: str):\n",
    "    # get top 3 results from knowledge base\n",
    "    results = vectorstore.similarity_search(query, k=3)\n",
    "    # get the text from the results\n",
    "    source_knowledge = \"\\n\".join([x.page_content for x in results])\n",
    "    # feed into an augmented prompt\n",
    "    augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
    "\n",
    "    Contexts:\n",
    "    {source_knowledge}\n",
    "\n",
    "    Query: {query}\"\"\"\n",
    "    \n",
    "    # save the source knowledge to a file in the specified folder\n",
    "    file_path = os.path.join(folder_path, 'context.txt')\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(source_knowledge)\n",
    "    \n",
    "    return augmented_prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the contexts below, answer the query.\n",
      "\n",
      "    Contexts:\n",
      "    Chatbots\\n\\nDocumentation\\n\\nEnd-to-end Example: Chat-LangChain\\n\\nÃ°\\x9fÂ¤\\x96 Agents\\n\\nDocumentation\\n\\nEnd-to-end Example: GPT+WolframAlpha\\n\\nÃ°\\x9fâ€œ\\x96 Documentation\\n\\nPlease see here for full documentation on:\\n\\nGetting started (installation, setting up the environment, simple examples)\\n\\nHow-To examples (demos, integrations, helper functions)\\n\\nReference (full API docs)\\n  Resources (high-level explanation of core concepts)\\n\\nÃ°\\x9f\\x9a\\x80 What can this help with?\\n\\nThere are six main areas that LangChain is designed to help with.\\nThese are, in increasing order of complexity:\\n\\nÃ°\\x9fâ€œ\\x83 LLMs and Prompts:\\n\\nThis includes prompt management, prompt optimization, generic interface for all LLMs, and common utilities for working with LLMs.\\n\\nÃ°\\x9fâ€\\x97 Chains:\\n\\nChains go beyond just a single LLM call, and are sequences of calls (whether to an LLM or a different utility). LangChain provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications.\\n\\nÃ°\\x9fâ€œ\\x9a\n",
      "chains for common applications.\\n\\nÃ°\\x9fâ€œ\\x9a Data Augmented Generation:\\n\\nData Augmented Generation involves specific types of chains that first interact with an external datasource to fetch data to use in the generation step. Examples of this include summarization of long pieces of text and question/answering over specific data sources.\\n\\nÃ°\\x9fÂ¤\\x96 Agents:\\n\\nAgents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done. LangChain provides a standard interface for agents, a selection of agents to choose from, and examples of end to end agents.\\n\\nÃ°\\x9fÂ§\\xa0 Memory:\\n\\nMemory is the concept of persisting state between calls of a chain/agent. LangChain provides a standard interface for memory, a collection of memory implementations, and examples of chains/agents that use memory.\\n\\nÃ°\\x9fÂ§\\x90 Evaluation:\\n\\n[BETA] Generative models are notoriously hard to evaluate with traditional metrics. One new way of evaluating them is using language models themselves to do the evaluation.\n",
      "[Document(page_content=\"Ã°\\x9fÂ¦\\x9cÃ¯Â¸\\x8fÃ°\\x9fâ€\\x97 LangChain\\n\\nÃ¢\\x9aÂ¡ Building applications with LLMs through composability Ã¢\\x9aÂ¡\\n\\nProduction Support: As you move your LangChains into production, we'd love to offer more comprehensive support.\\nPlease fill out this form and we'll set up a dedicated support Slack channel.\\n\\nQuick Install\\n\\npip install langchain\\n\\nÃ°\\x9fÂ¤â€ What is this?\\n\\nLarge language models (LLMs) are emerging as a transformative technology, enabling\\ndevelopers to build applications that they previously could not.\\nBut using these LLMs in isolation is often not enough to\\ncreate a truly powerful app - the real power comes when you can combine them with other sources of computation or knowledge.\\n\\nThis library is aimed at assisting in the development of those types of applications. Common examples of these types of applications include:\\n\\nÃ¢\\x9dâ€œ Question Answering over specific documents\\n\\nDocumentation\\n\\nEnd-to-end Example: Question Answering over Notion Database\\n\\nÃ°\\x9fâ€™Â¬ Chatbots\\n\\nDocumentation\\n\\nEnd-to-end Example:\n",
      "\n",
      "    Query: what is langchain\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"../prompts\"\n",
    "print(augment_prompt(query, folder_path))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this we produce an augmented prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the contexts below, answer the query.\n",
      "\n",
      "    Contexts:\n",
      "    Chatbots\\n\\nDocumentation\\n\\nEnd-to-end Example: Chat-LangChain\\n\\nÃ°\\x9fÂ¤\\x96 Agents\\n\\nDocumentation\\n\\nEnd-to-end Example: GPT+WolframAlpha\\n\\nÃ°\\x9fâ€œ\\x96 Documentation\\n\\nPlease see here for full documentation on:\\n\\nGetting started (installation, setting up the environment, simple examples)\\n\\nHow-To examples (demos, integrations, helper functions)\\n\\nReference (full API docs)\\n  Resources (high-level explanation of core concepts)\\n\\nÃ°\\x9f\\x9a\\x80 What can this help with?\\n\\nThere are six main areas that LangChain is designed to help with.\\nThese are, in increasing order of complexity:\\n\\nÃ°\\x9fâ€œ\\x83 LLMs and Prompts:\\n\\nThis includes prompt management, prompt optimization, generic interface for all LLMs, and common utilities for working with LLMs.\\n\\nÃ°\\x9fâ€\\x97 Chains:\\n\\nChains go beyond just a single LLM call, and are sequences of calls (whether to an LLM or a different utility). LangChain provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications.\\n\\nÃ°\\x9fâ€œ\\x9a\n",
      "chains for common applications.\\n\\nÃ°\\x9fâ€œ\\x9a Data Augmented Generation:\\n\\nData Augmented Generation involves specific types of chains that first interact with an external datasource to fetch data to use in the generation step. Examples of this include summarization of long pieces of text and question/answering over specific data sources.\\n\\nÃ°\\x9fÂ¤\\x96 Agents:\\n\\nAgents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done. LangChain provides a standard interface for agents, a selection of agents to choose from, and examples of end to end agents.\\n\\nÃ°\\x9fÂ§\\xa0 Memory:\\n\\nMemory is the concept of persisting state between calls of a chain/agent. LangChain provides a standard interface for memory, a collection of memory implementations, and examples of chains/agents that use memory.\\n\\nÃ°\\x9fÂ§\\x90 Evaluation:\\n\\n[BETA] Generative models are notoriously hard to evaluate with traditional metrics. One new way of evaluating them is using language models themselves to do the evaluation.\n",
      "[Document(page_content=\"Ã°\\x9fÂ¦\\x9cÃ¯Â¸\\x8fÃ°\\x9fâ€\\x97 LangChain\\n\\nÃ¢\\x9aÂ¡ Building applications with LLMs through composability Ã¢\\x9aÂ¡\\n\\nProduction Support: As you move your LangChains into production, we'd love to offer more comprehensive support.\\nPlease fill out this form and we'll set up a dedicated support Slack channel.\\n\\nQuick Install\\n\\npip install langchain\\n\\nÃ°\\x9fÂ¤â€ What is this?\\n\\nLarge language models (LLMs) are emerging as a transformative technology, enabling\\ndevelopers to build applications that they previously could not.\\nBut using these LLMs in isolation is often not enough to\\ncreate a truly powerful app - the real power comes when you can combine them with other sources of computation or knowledge.\\n\\nThis library is aimed at assisting in the development of those types of applications. Common examples of these types of applications include:\\n\\nÃ¢\\x9dâ€œ Question Answering over specific documents\\n\\nDocumentation\\n\\nEnd-to-end Example: Question Answering over Notion Database\\n\\nÃ°\\x9fâ€™Â¬ Chatbots\\n\\nDocumentation\\n\\nEnd-to-end Example:\n",
      "\n",
      "    Query: what is langchain\n"
     ]
    }
   ],
   "source": [
    "print(augment_prompt(query))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is still a lot of text here, so let's pass it onto our chat model to see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is a library aimed at assisting developers in building applications with Large Language Models (LLMs) through composability. LLMs are powerful language models that enable developers to create applications that combine them with other sources of computation or knowledge. LangChain provides a standard interface for various features such as prompt management, chains (sequences of calls), data augmented generation, agents (LLMs making decisions), memory (persisting state), and evaluation. It offers integrations with other tools and examples of end-to-end applications like chatbots and question answering over specific documents.\n"
     ]
    }
   ],
   "source": [
    "# create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=augment_prompt(query)\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "res = chat(messages)\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can continue with more Llama 2 questions. Let's try _without_ RAG first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the context of LangChain, Agents refer to a concept where a large language model (LLM) makes decisions about which actions to take, takes those actions, observes the outcomes, and repeats the process until a desired outcome is achieved. Agents essentially use LLMs to perform tasks that involve decision-making and interaction with the environment.\n",
      "\n",
      "LangChain provides a standard interface for agents, a selection of pre-built agents to choose from, and examples of end-to-end agent applications. These agents can be used to build applications that require the LLM to perform sequential actions based on the observed outcomes.\n",
      "\n",
      "By leveraging the capabilities of LLMs and the agent framework provided by LangChain, developers can create more complex and interactive applications that involve decision-making, such as chatbots, virtual assistants, game characters, and more.\n"
     ]
    }
   ],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=\"what are Agents in langchain?\"\n",
    ")\n",
    "\n",
    "res = chat(messages + [prompt])\n",
    "print(res.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chatbot is able to respond about Llama 2 thanks to it's conversational history stored in `messages`. However, it doesn't know anything about the safety measures themselves as we have not provided it with that information via the RAG pipeline. Let's try again but with RAG."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a much more informed response that includes several items missing in the previous non-RAG response, such as \"red-teaming\", \"iterative evaluations\", and the intention of the researchers to share this research to help \"improve their safety, promoting responsible development in the field\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
