{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "\n",
    "os.path.abspath(os.path.join('../Scripts'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Scripts'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mScripts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ranking\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Scripts'"
     ]
    }
   ],
   "source": [
    "from Scripts.ranking import evaluate_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\\n    {\\n        \"prompt\": \"What is the purpose of Llama 2?\",\\n        \"ground_truth\": \"Llama 2 is a collection of pretrained and fine-tuned large language models optimized for dialogue use cases.\"\\n    },\\n    {\\n        \"prompt\": \"How do Llama 2 models perform compared to open-source chat models?\",\\n        \"ground_truth\": \"Llama 2 models generally outperform open-source chat models on most benchmarks.\"\\n    },\\n    {\\n        \"prompt\": \"Are Llama 2 models a suitable substitute for closed-source models?\",\\n        \"ground_truth\": \"Based on humane evaluations for helpfulness and safety, Llama 2 models may be a suitable substitute for closed-source models.\"\\n    },\\n    {\\n        \"prompt\": \"What is the approach to fine-tuning and safety in Llama 2?\",\\n        \"ground_truth\": \"A detailed description of the approach to fine-tuning and safety in Llama 2 is provided, similar to ChatGPT, BARD, and Claude.\"\\n    },\\n    {\\n        \"prompt\": \"What limits progress in AI alignment research within the community?\",\\n        \"ground_truth\": \"The lack of transparency and reproducibility in the fine-tuning process of closed-source models limits progress in AI alignment research within the community.\"\\n    }\\n]'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = '../test-dataset/test-data.json'\n",
    "# Open the file in read mode\n",
    "with open(file_path, 'r') as file:\n",
    "    # Read the contents of the file\n",
    "    file_contents = file.read()\n",
    "file_contents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\\nRoss Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\\nAngela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\\nSergey Edunov Thomas Scialom\\x03\\nGenAI, Meta\\nAbstract\\nIn this work, we develop and release Llama 2, a collection of pretrained and ﬁne-tuned\\nlarge language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\\nOur ﬁne-tuned LLMs, called L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , are optimized for dialogue use cases. Our\\nmodels outperform open-source chat models on most benchmarks we tested, and based on\\nourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosedsource models. We provide a detailed description of our approach to ﬁne-tuning and safety\\nasChatGPT,BARD,andClaude. TheseclosedproductLLMsareheavilyﬁne-tunedtoalignwithhuman\\npreferences, which greatly enhances their usability and safety. This step can require signiﬁcant costs in\\ncomputeandhumanannotation,andisoftennottransparentoreasilyreproducible,limitingprogresswithin\\nthe community to advance AI alignment research.\\nIn this work, we develop and release Llama 2, a family of pretrained and ﬁne-tuned LLMs, L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle and\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc models generally perform better than existing open-source models. They also appear to\\nbe on par with some of the closed-source models, at least on the human evaluations we performed (see\\nA2qXzbHspgrZJIa0/PcFIOcahRM8lmlnxmeUjahI96zVNGYmyBfLDsjF1YZkijR9ikkC/V3IqexMdM4tJMxxbFZ9ebif14vw+gmyIVKM+SKLT+KMkkwIfPLyVBozlBOLaFMC7srYWOqKUPbT8WW4K2evE7aV3XPrXsP17XmbVFHGc7gHC7BgwY04R5a4AMDAc/wCm+Ocl6cd+djOVpyiswp/IHz+QPvmo68</latexit>G1'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = '../prompts/context.txt'\n",
    "# Open the file in read mode\n",
    "with open(file_path, 'r') as file:\n",
    "    # Read the contents of the file\n",
    "    context = file.read()\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# file_contents = file_contents\n",
    "\n",
    "# Load the JSON data\n",
    "data = json.loads(file_contents)\n",
    "\n",
    "# Separate prompts and ground truths\n",
    "questions = [item['prompt'] for item in data]\n",
    "ground_truths = [item['ground_truth'] for item in data]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ground Truths:\n",
      "Llama 2 is a collection of pretrained and fine-tuned large language models optimized for dialogue use cases.\n",
      "Llama 2 models generally outperform open-source chat models on most benchmarks.\n",
      "Based on humane evaluations for helpfulness and safety, Llama 2 models may be a suitable substitute for closed-source models.\n",
      "A detailed description of the approach to fine-tuning and safety in Llama 2 is provided, similar to ChatGPT, BARD, and Claude.\n",
      "The lack of transparency and reproducibility in the fine-tuning process of closed-source models limits progress in AI alignment research within the community.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nGround Truths:\")\n",
    "for ground_truth in ground_truths:\n",
    "    print(ground_truth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "questions:\n",
      "What is the purpose of Llama 2?\n",
      "How do Llama 2 models perform compared to open-source chat models?\n",
      "Are Llama 2 models a suitable substitute for closed-source models?\n",
      "What is the approach to fine-tuning and safety in Llama 2?\n",
      "What limits progress in AI alignment research within the community?\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nquestions:\")\n",
    "for question in questions:\n",
    "    print(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What is the purpose of Llama 2?',\n",
       " 'How do Llama 2 models perform compared to open-source chat models?',\n",
       " 'Are Llama 2 models a suitable substitute for closed-source models?',\n",
       " 'What is the approach to fine-tuning and safety in Llama 2?',\n",
       " 'What limits progress in AI alignment research within the community?']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_prompt = \"what is special about lama2?\"\n",
    "test_cases = questions\n",
    "# result = evaluate_prompt(main_prompt, test_cases)\n",
    "# print(result)\n",
    "test_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'main_prompt': {'Monte Carlo Evaluation': 2.05, 'Elo Rating Evaluation': 1504.2019499940866}, 'test_case_1': {'Monte Carlo Evaluation': 2.0, 'Elo Rating Evaluation': 1504.2019499940866}, 'test_case_2': {'Monte Carlo Evaluation': 1.99, 'Elo Rating Evaluation': 1519.2019499940866}, 'test_case_3': {'Monte Carlo Evaluation': 2.14, 'Elo Rating Evaluation': 1504.2019499940866}, 'test_case_4': {'Monte Carlo Evaluation': 1.88, 'Elo Rating Evaluation': 1519.2019499940866}, 'test_case_5': {'Monte Carlo Evaluation': 2.04, 'Elo Rating Evaluation': 1504.2019499940866}}\n"
     ]
    }
   ],
   "source": [
    "result = evaluate_prompt(main_prompt, test_cases)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'main_prompt': {'Monte Carlo Evaluation': 2.08, 'Elo Rating Evaluation': 1519.2019499940866}, 'test_case_1': {'Monte Carlo Evaluation': 2.05, 'Elo Rating Evaluation': 1504.2019499940866}, 'test_case_2': {'Monte Carlo Evaluation': 2.05, 'Elo Rating Evaluation': 1519.2019499940866}, 'test_case_3': {'Monte Carlo Evaluation': 1.97, 'Elo Rating Evaluation': 1519.2019499940866}, 'test_case_4': {'Monte Carlo Evaluation': 2.1, 'Elo Rating Evaluation': 1489.2019499940866}, 'test_case_5': {'Monte Carlo Evaluation': 1.99, 'Elo Rating Evaluation': 1489.2019499940866}}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAGAS Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter  \n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Weaviate\n",
    "# import weaviate\n",
    "# from weaviate.embedded import EmbeddedOptions\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Weaviate\n",
    "# import weaviate\n",
    "# from weaviate.embedded import EmbeddedOptions\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "# \n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What is the purpose of Llama 2?',\n",
       " 'How do Llama 2 models perform compared to open-source chat models?',\n",
       " 'Are Llama 2 models a suitable substitute for closed-source models?']"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_list=questions[:3] \n",
    "questions_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truths (List of Lists):\n",
      "['Llama 2 is a collection of pretrained and fine-tuned large language models optimized for dialogue use cases.']\n",
      "['Llama 2 models generally outperform open-source chat models on most benchmarks.']\n",
      "['Based on humane evaluations for helpfulness and safety, Llama 2 models may be a suitable substitute for closed-source models.']\n"
     ]
    }
   ],
   "source": [
    "ground_truths_lists = [[item] for item in ground_truths[:3]]\n",
    "\n",
    "# Print the modified list of lists\n",
    "print(\"Ground Truths (List of Lists):\")\n",
    "for sublist in ground_truths_lists:\n",
    "    print(sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang', 'Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang', 'Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic', 'Sergey Edunov Thomas Scialom\\x03', 'GenAI, Meta', 'Abstract']\n",
      "['In this work, we develop and release Llama 2, a collection of pretrained and ﬁne-tuned', 'large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.', 'Our ﬁne-tuned LLMs, called L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , are optimized for dialogue use cases. Our', 'models outperform open-source chat models on most benchmarks we tested, and based on', 'ourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosedsource models. We provide a detailed description of our approach to ﬁne-tuning and safety', 'asChatGPT,BARD,andClaude. TheseclosedproductLLMsareheavilyﬁne-tunedtoalignwithhuman', 'preferences, which greatly enhances their usability and safety. This step can require signiﬁcant costs in', 'computeandhumanannotation,andisoftennottransparentoreasilyreproducible,limitingprogresswithin', 'the community to advance AI alignment research.']\n",
      "['In this work, we develop and release Llama 2, a family of pretrained and ﬁne-tuned LLMs, L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle and', 'L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,', 'L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc models generally perform better than existing open-source models. They also appear to', 'be on par with some of the closed-source models, at least on the human evaluations we performed']\n"
     ]
    }
   ],
   "source": [
    "context_lists = [\n",
    "    [\"Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\",\n",
    "    \"Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\",\n",
    "    \"Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\",\n",
    "    \"Sergey Edunov Thomas Scialom\\x03\",\n",
    "    \"GenAI, Meta\",\n",
    "    \"Abstract\"],\n",
    "    \n",
    "    [\"In this work, we develop and release Llama 2, a collection of pretrained and ﬁne-tuned\",\n",
    "    \"large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\",\n",
    "    \"Our ﬁne-tuned LLMs, called L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , are optimized for dialogue use cases. Our\",\n",
    "    \"models outperform open-source chat models on most benchmarks we tested, and based on\",\n",
    "    \"ourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosedsource models. We provide a detailed description of our approach to ﬁne-tuning and safety\",\n",
    "    \"asChatGPT,BARD,andClaude. TheseclosedproductLLMsareheavilyﬁne-tunedtoalignwithhuman\",\n",
    "    \"preferences, which greatly enhances their usability and safety. This step can require signiﬁcant costs in\",\n",
    "    \"computeandhumanannotation,andisoftennottransparentoreasilyreproducible,limitingprogresswithin\",\n",
    "    \"the community to advance AI alignment research.\"],\n",
    "\n",
    "    [\"In this work, we develop and release Llama 2, a family of pretrained and ﬁne-tuned LLMs, L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle and\",\n",
    "    \"L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\",\n",
    "    \"L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc models generally perform better than existing open-source models. They also appear to\",\n",
    "    \"be on par with some of the closed-source models, at least on the human evaluations we performed\"]\n",
    "]\n",
    "\n",
    "# Print the list of lists\n",
    "for sublist in context_lists:\n",
    "    print(sublist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The purpose of Llama 2 is to provide a collection of pretrained and fine-tuned large language models optimized for dialogue use cases, outperforming open-source chat models and potentially serving as a substitute for closed-source models.', 'Llama 2 models generally perform better than existing open-source chat models.', 'Yes, Llama 2 models generally perform better than existing open-source models and appear to be on par with some closed-source models based on human evaluations.']\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load Answer list from the file\n",
    "with open('answer_list.pkl', 'rb') as file:\n",
    "    Answer = pickle.load(file)\n",
    "\n",
    "# Now, Answer contains the list of responses from the first notebook\n",
    "print(Answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "data = {\n",
    "    \"question\": questions_list, # list \n",
    "    \"answer\": Answer, # list\n",
    "    \"contexts\": context_lists, # list list\n",
    "    \"ground_truths\": ground_truths_lists # list Lists\n",
    "}\n",
    "\n",
    "# Convert dict to dataset\n",
    "dataset = Dataset.from_dict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_precision]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:06<00:00,  6.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_recall]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [faithfulness]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [answer_relevancy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.30s/it]\n"
     ]
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "\n",
    "result = evaluate(\n",
    "    dataset = dataset, \n",
    "    metrics=[\n",
    "        context_precision,\n",
    "        context_recall,\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "    ],\n",
    ")\n",
    "\n",
    "df = result.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truths</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the purpose of Llama 2?</td>\n",
       "      <td>The purpose of Llama 2 is to provide a collect...</td>\n",
       "      <td>[Alan Schelten Ruan Silva Eric Michael Smith R...</td>\n",
       "      <td>[Llama 2 is a collection of pretrained and fin...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How do Llama 2 models perform compared to open...</td>\n",
       "      <td>Llama 2 models generally perform better than e...</td>\n",
       "      <td>[In this work, we develop and release Llama 2,...</td>\n",
       "      <td>[Llama 2 models generally outperform open-sour...</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.979799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Are Llama 2 models a suitable substitute for c...</td>\n",
       "      <td>Yes, Llama 2 models generally perform better t...</td>\n",
       "      <td>[In this work, we develop and release Llama 2,...</td>\n",
       "      <td>[Based on humane evaluations for helpfulness a...</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.933559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0                    What is the purpose of Llama 2?   \n",
       "1  How do Llama 2 models perform compared to open...   \n",
       "2  Are Llama 2 models a suitable substitute for c...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  The purpose of Llama 2 is to provide a collect...   \n",
       "1  Llama 2 models generally perform better than e...   \n",
       "2  Yes, Llama 2 models generally perform better t...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [Alan Schelten Ruan Silva Eric Michael Smith R...   \n",
       "1  [In this work, we develop and release Llama 2,...   \n",
       "2  [In this work, we develop and release Llama 2,...   \n",
       "\n",
       "                                       ground_truths  context_precision  \\\n",
       "0  [Llama 2 is a collection of pretrained and fin...               0.00   \n",
       "1  [Llama 2 models generally outperform open-sour...               0.25   \n",
       "2  [Based on humane evaluations for helpfulness a...               0.25   \n",
       "\n",
       "   context_recall  faithfulness  answer_relevancy  \n",
       "0             1.0           0.0          1.000000  \n",
       "1             1.0           1.0          0.979799  \n",
       "2             1.0           1.0          0.933559  "
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Integration with Retrieval-Augmented Generation Assessment:\n",
    "##### Monte Carlo for Robustness Testing: Use Monte Carlo simulations to test the robustness of the RAG system across a wide range of possible retrieval scenarios. This helps in understanding how different types of retrieved information can impact the quality of the generated content.\n",
    "##### Elo Rating for Continuous Improvement: Utilize the Elo rating system to continuously assess and improve the RAG model. By comparing new outputs with previous ones and adjusting ratings accordingly, the system can learn which types of retrieval-augmented generations work best."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7f3c0b24c41e80e94b84b01069c679ceb9a0604be81ab76bb66c9ea948f7d76b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
